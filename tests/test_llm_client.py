"""–¢–µ—Å—Ç—ã –¥–ª—è LLM –∫–ª–∏–µ–Ω—Ç–∞."""
import pytest
import asyncio
import aiohttp
from unittest.mock import patch, AsyncMock, mock_open
from src.llm.client import LLMClient


class TestLLMClient:
    """–¢–µ—Å—Ç—ã –¥–ª—è –∫–ª–∞—Å—Å–∞ LLMClient."""
    
    @pytest.fixture
    def llm_client(self, mock_settings) -> LLMClient:
        """–§–∏–∫—Å—Ç—É—Ä–∞ LLM –∫–ª–∏–µ–Ω—Ç–∞ —Å –º–æ–∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏."""
        with patch("builtins.open", mock_open(read_data="–¢–µ—Å—Ç–æ–≤—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç")):
            return LLMClient()
    
    def test_init(self, llm_client: LLMClient, mock_settings) -> None:
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞."""
        assert llm_client.api_url == "https://openrouter.ai/api/v1/chat/completions"
        assert "Authorization" in llm_client.headers
        assert llm_client.headers["Authorization"].startswith("Bearer ")
        assert llm_client.headers["Content-Type"] == "application/json"
        assert llm_client.system_prompt is not None
    
    def test_load_system_prompt_success(self, mock_settings) -> None:
        """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞."""
        test_prompt = "–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç"
        with patch("builtins.open", mock_open(read_data=test_prompt)):
            client = LLMClient()
            assert client.system_prompt == test_prompt
    
    def test_load_system_prompt_file_not_found(self, mock_settings) -> None:
        """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ñ–∞–π–ª–∞."""
        with patch("builtins.open", side_effect=FileNotFoundError()):
            client = LLMClient()
            assert "—Å–∞—Ä–∫–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç" in client.system_prompt.lower()
    
    def test_get_default_prompt(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞."""
        default_prompt = llm_client._get_default_prompt()
        assert "—Å–∞—Ä–∫–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç" in default_prompt.lower()
        assert "—Å–∞—Ä–∫–∞–∑–º" in default_prompt.lower()
    
    def test_prepare_payload_without_context(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ payload –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        user_message = "–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
        payload = llm_client._prepare_payload(user_message)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        assert "model" in payload
        assert payload["temperature"] == 0.8
        assert payload["max_tokens"] == 500
        assert len(payload["messages"]) == 2  # system + user
        assert payload["messages"][0]["role"] == "system"
        assert payload["messages"][1]["role"] == "user"
        assert payload["messages"][1]["content"] == user_message
    
    def test_prepare_payload_with_context(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ payload —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""
        user_message = "–ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
        context_messages = [
            {"role": "user", "content": "–ü—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"},
            {"role": "assistant", "content": "–ü—Ä–µ–¥—ã–¥—É—â–∏–π –æ—Ç–≤–µ—Ç"}
        ]
        
        payload = llm_client._prepare_payload(user_message, context_messages)
        
        assert len(payload["messages"]) == 4  # system + context + user
        assert payload["messages"][0]["role"] == "system"
        assert payload["messages"][1] == context_messages[0]
        assert payload["messages"][2] == context_messages[1] 
        assert payload["messages"][3]["role"] == "user"
        assert payload["messages"][3]["content"] == user_message
    
    def test_prepare_payload_limits_context(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ."""
        user_message = "–ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
        # –°–æ–∑–¥–∞–µ–º 30 —Å–æ–æ–±—â–µ–Ω–∏–π (–±–æ–ª—å—à–µ –ª–∏–º–∏—Ç–∞ –≤ 20)
        context_messages = []
        for i in range(30):
            context_messages.append({"role": "user", "content": f"–°–æ–æ–±—â–µ–Ω–∏–µ {i}"})
        
        payload = llm_client._prepare_payload(user_message, context_messages)
        
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ –±–æ–ª—å—à–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π: system + context (max 19) + user (1)
        assert len(payload["messages"]) == 20
        assert payload["messages"][0]["role"] == "system"
        assert payload["messages"][-1]["role"] == "user"
        assert payload["messages"][-1]["content"] == user_message
    
    def test_get_max_context_messages(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π."""
        max_messages = llm_client._get_max_context_messages()
        assert max_messages == 20
    
    @pytest.mark.asyncio
    async def test_make_request_success(self, llm_client: LLMClient, mock_aiohttp_session, mock_openrouter_response) -> None:
        """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ HTTP –∑–∞–ø—Ä–æ—Å–∞."""
        payload = {"test": "payload"}
        
        result = await llm_client._make_request(payload)
        
        assert result == "–¢–µ—Å—Ç–æ–≤—ã–π —Å–∞—Ä–∫–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò"
    
    @pytest.mark.asyncio 
    async def test_make_request_rate_limit_error(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–∫–∏ rate limit."""
        payload = {"test": "payload"}
        
        with patch("aiohttp.ClientSession") as mock_session:
            mock_response = AsyncMock()
            mock_response.status = 429
            mock_session.return_value.__aenter__.return_value.post.return_value.__aenter__.return_value = mock_response
            
            with pytest.raises(Exception, match="Rate limit exceeded"):
                await llm_client._make_request(payload)
    
    @pytest.mark.asyncio
    async def test_make_request_auth_error(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–∫–∏ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏."""
        payload = {"test": "payload"}
        
        with patch("aiohttp.ClientSession") as mock_session:
            mock_response = AsyncMock()
            mock_response.status = 401
            mock_session.return_value.__aenter__.return_value.post.return_value.__aenter__.return_value = mock_response
            
            with pytest.raises(Exception, match="Invalid API key"):
                await llm_client._make_request(payload)
    
    @pytest.mark.asyncio
    async def test_make_request_server_error(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–µ—Ä–≤–µ—Ä–Ω–æ–π –æ—à–∏–±–∫–∏."""
        payload = {"test": "payload"}
        
        with patch("aiohttp.ClientSession") as mock_session:
            mock_response = AsyncMock()
            mock_response.status = 500
            mock_response.text.return_value = "Internal Server Error"
            mock_session.return_value.__aenter__.return_value.post.return_value.__aenter__.return_value = mock_response
            
            with pytest.raises(Exception, match="API error 500"):
                await llm_client._make_request(payload)
    
    def test_classify_error_timeout(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ timeout –æ—à–∏–±–æ–∫."""
        error = Exception("Connection timeout occurred")
        error_type = llm_client._classify_error(error)
        assert error_type == "timeout"
    
    def test_classify_error_rate_limit(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ rate limit –æ—à–∏–±–æ–∫."""
        error = Exception("Rate limit exceeded")
        error_type = llm_client._classify_error(error)
        assert error_type == "rate_limit"
    
    def test_classify_error_auth(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ—à–∏–±–æ–∫ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏."""
        error = Exception("Invalid API key provided")
        error_type = llm_client._classify_error(error)
        assert error_type == "auth_error"
    
    def test_classify_error_network(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫."""
        error = Exception("Connection error occurred")
        error_type = llm_client._classify_error(error)
        assert error_type == "network_error"
    
    def test_classify_error_server(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–µ—Ä–≤–µ—Ä–Ω—ã—Ö –æ—à–∏–±–æ–∫."""
        error = Exception("Server error 500")
        error_type = llm_client._classify_error(error)
        assert error_type == "server_error"
    
    def test_classify_error_unknown(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ—à–∏–±–æ–∫."""
        error = Exception("Some random error")
        error_type = llm_client._classify_error(error)
        assert error_type == "unknown"
    
    def test_get_fallback_response_timeout(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è fallback –æ—Ç–≤–µ—Ç–∞ –¥–ª—è timeout."""
        response = llm_client._get_fallback_response("timeout")
        assert "‚è∞" in response or "üïí" in response or "‚åõ" in response
        assert any(word in response.lower() for word in ["–≤—Ä–µ–º—è", "—Ç–∞–π–º–∞—É—Ç", "–¥–æ–ª–≥–æ"])
    
    def test_get_fallback_response_rate_limit(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è fallback –æ—Ç–≤–µ—Ç–∞ –¥–ª—è rate limit."""
        response = llm_client._get_fallback_response("rate_limit")
        assert "üö¶" in response or "üìä" in response or "üéØ" in response
        assert any(word in response.lower() for word in ["–ª–∏–º–∏—Ç", "–ø—Ä–µ–≤—ã—Å", "–º–Ω–æ–≥–æ"])
    
    def test_get_fallback_response_auth_error(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è fallback –æ—Ç–≤–µ—Ç–∞ –¥–ª—è auth error."""
        response = llm_client._get_fallback_response("auth_error")
        assert "üîë" in response or "üö™" in response
        assert any(word in response.lower() for word in ["–∞–≤—Ç–æ—Ä–∏–∑", "–∫–ª—é—á", "–ø—É—Å–∫"])
    
    def test_get_fallback_response_unknown(self, llm_client: LLMClient) -> None:
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è fallback –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–π –æ—à–∏–±–∫–∏."""
        response = llm_client._get_fallback_response("unknown")
        assert len(response) > 0
        assert any(emoji in response for emoji in ["ü§ñ", "üí•", "üé≠"])
    
    @pytest.mark.asyncio
    async def test_send_message_success(self, llm_client: LLMClient, mock_aiohttp_session, mock_logger) -> None:
        """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è."""
        user_message = "–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
        user_id = "test_user"
        
        result = await llm_client.send_message(user_message, None, user_id)
        
        assert result == "–¢–µ—Å—Ç–æ–≤—ã–π —Å–∞—Ä–∫–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò"
        mock_logger.info.assert_called()
        mock_logger.log_llm_request.assert_called()
    
    @pytest.mark.asyncio
    async def test_send_message_with_context(self, llm_client: LLMClient, mock_aiohttp_session, mock_logger) -> None:
        """–¢–µ—Å—Ç –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º."""
        user_message = "–ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
        context_messages = [{"role": "user", "content": "–°—Ç–∞—Ä–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"}]
        user_id = "test_user"
        
        result = await llm_client.send_message(user_message, context_messages, user_id)
        
        assert result == "–¢–µ—Å—Ç–æ–≤—ã–π —Å–∞—Ä–∫–∞—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò"
        mock_logger.log_llm_request.assert_called()
    
    @pytest.mark.asyncio
    async def test_send_message_with_retry(self, llm_client: LLMClient, mock_logger) -> None:
        """–¢–µ—Å—Ç retry –ª–æ–≥–∏–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö."""
        user_message = "–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
        user_id = "test_user"
        
        with patch.object(llm_client, "_make_request") as mock_request:
            # –ü–µ—Ä–≤—ã–µ 2 –ø–æ–ø—ã—Ç–∫–∏ - –æ—à–∏–±–∫–∞, —Ç—Ä–µ—Ç—å—è - —É—Å–ø–µ—Ö
            mock_request.side_effect = [
                Exception("Network error"),
                Exception("Timeout error"), 
                "–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç"
            ]
            
            result = await llm_client.send_message(user_message, None, user_id)
            
            assert result == "–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç"
            assert mock_request.call_count == 3
            mock_logger.log_llm_error.assert_called()
    
    @pytest.mark.asyncio
    async def test_send_message_all_retries_failed(self, llm_client: LLMClient, mock_logger) -> None:
        """–¢–µ—Å—Ç —Å–ª—É—á–∞—è –∫–æ–≥–¥–∞ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å."""
        user_message = "–¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"
        user_id = "test_user"
        
        with patch.object(llm_client, "_make_request") as mock_request:
            mock_request.side_effect = Exception("Persistent error")
            
            result = await llm_client.send_message(user_message, None, user_id)
            
            # –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å—Å—è fallback –æ—Ç–≤–µ—Ç
            assert len(result) > 0
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 3 (–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ LLM_RETRY_ATTEMPTS)
            assert mock_request.call_count == 3
            mock_logger.log_llm_error.assert_called()
            mock_logger.error.assert_called()
